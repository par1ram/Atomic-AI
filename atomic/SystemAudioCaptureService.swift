//
//  SystemAudioCaptureService.swift
//  atomic
//
//  –°–µ—Ä–≤–∏—Å –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∞—É–¥–∏–æ (–∑–≤—É–∫ –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞) —Å –Ω–∞—Ç–∏–≤–Ω—ã–º SFSpeechRecognizer
//  –ë–´–°–¢–†–´–ô: ~100-200ms –∑–∞–¥–µ—Ä–∂–∫–∞ (–≤ 5-10 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ WhisperKit)

import Foundation
import ScreenCaptureKit
import Speech
import AVFoundation

class SystemAudioCaptureService: NSObject {
    // MARK: - Constants

    private enum Constants {
        static let locale = Locale(identifier: "ru-RU")
        static let silenceThreshold: TimeInterval = 1.5
        static let pauseCheckInterval: TimeInterval = 0.5
    }

    // MARK: - Properties

    private var stream: SCStream?
    private let speechRecognizer = SFSpeechRecognizer(locale: Constants.locale)
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    private var latestTranscript = ""
    private var fullAccumulatedText = "" // –ü–æ–ª–Ω—ã–π –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç SFSpeech
    private var isFinalTranscript = false

    // –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–∞—É–∑ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    private var lastResultTime: Date?

    func startCapture() async throws {
        // –ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∞—É–¥–∏–æ
        let content = try await SCShareableContent.current

        guard let display = content.displays.first else {
            throw NSError(domain: "SystemAudio", code: 1, userInfo: [NSLocalizedDescriptionKey: "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∏—Å–ø–ª–µ–µ–≤"])
        }

        // –°–æ–∑–¥–∞—Ç—å —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –í–°–ï–• –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π (—Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫)
        // –ò—Å–∫–ª—é—á–∞–µ–º —Å–µ–±—è, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø–µ—Ç–ª–∏
        let excludedApps = content.applications.filter { app in
            app.bundleIdentifier == Bundle.main.bundleIdentifier
        }

        let filter = SCContentFilter(display: display, excludingApplications: excludedApps, exceptingWindows: [])

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∞—É–¥–∏–æ
        let config = SCStreamConfiguration()

        // –ö–õ–Æ–ß–ï–í–û–ï: –≤–∫–ª—é—á–∞–µ–º –∑–∞—Ö–≤–∞—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∞—É–¥–∏–æ
        config.capturesAudio = true
        config.excludesCurrentProcessAudio = true  // –ò—Å–∫–ª—é—á–∞–µ–º —Å–≤–æ–π –∑–≤—É–∫
        config.sampleRate = 48000
        config.channelCount = 2

        // –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∏–¥–µ–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (ScreenCaptureKit —Ç—Ä–µ–±—É–µ—Ç —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ)
        // –î–µ–ª–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π framerate —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –æ—à–∏–±–∫–∏ "output NOT found"
        config.width = 1
        config.height = 1
        config.pixelFormat = kCVPixelFormatType_32BGRA
        config.minimumFrameInterval = CMTime(value: 1, timescale: 1) // 1 fps
        config.queueDepth = 1

        // –°–æ–∑–¥–∞—Ç—å stream
        stream = SCStream(filter: filter, configuration: config, delegate: self)

        // –î–æ–±–∞–≤–∏—Ç—å –∞—É–¥–∏–æ output
        try stream?.addStreamOutput(self, type: .audio, sampleHandlerQueue: DispatchQueue(label: "audio.capture.queue"))

        // –ó–∞–ø—É—Å—Ç–∏—Ç—å
        try await stream?.startCapture()

        // –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
        startSpeechRecognition()
    }

    func stopCapture() async {
        try? await stream?.stopCapture()
        stream = nil

        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
    }

    private func startSpeechRecognition() {
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true

        guard let recognitionRequest = recognitionRequest else { return }

        recognitionTask = setupRecognitionTask(with: recognitionRequest)

        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—É–∑ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        Timer.scheduledTimer(withTimeInterval: Constants.pauseCheckInterval, repeats: true) { [weak self] timer in
            guard let self = self else {
                timer.invalidate()
                return
            }

            // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¢–û–õ–¨–ö–û –û–î–ò–ù –†–ê–ó –ø—Ä–∏ –ø–∞—É–∑–µ
            if let lastTime = self.lastResultTime,
               Date().timeIntervalSince(lastTime) > Constants.silenceThreshold,
               !self.isFinalTranscript,
               !self.latestTranscript.isEmpty {
                self.isFinalTranscript = true
                print("‚úÖ [–ò–Ω—Ç–µ—Ä–≤—å—é–µ—Ä] –§—Ä–∞–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (–ø–∞—É–∑–∞)")

                // –†–µ—Å—Ç–∞—Ä—Ç—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞
                self.restartRecognition()
            }
        }
    }

    private func restartRecognition() {
        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()

        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        fullAccumulatedText = ""

        // –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true

        guard let recognitionRequest = recognitionRequest else { return }

        recognitionTask = setupRecognitionTask(with: recognitionRequest)

        print("üîÑ SFSpeech —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–æ")
    }

    // MARK: - Shared Recognition Task Setup

    private func setupRecognitionTask(with request: SFSpeechAudioBufferRecognitionRequest) -> SFSpeechRecognitionTask? {
        return speechRecognizer?.recognitionTask(with: request) { [weak self] result, error in
            guard let self = self else { return }

            if let result = result {
                let transcript = result.bestTranscription.formattedString
                    .trimmingCharacters(in: .whitespacesAndNewlines)

                // –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∞–º)
                if transcript != self.fullAccumulatedText && !transcript.isEmpty {
                    // –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ù–û–í–£–Æ —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–≤—ã—á–∏—Ç–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π)
                    let previousText = self.fullAccumulatedText
                    self.fullAccumulatedText = transcript

                    // –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—É—é —á–∞—Å—Ç—å
                    let newPart = transcript.hasPrefix(previousText) && !previousText.isEmpty
                        ? String(transcript.dropFirst(previousText.count)).trimmingCharacters(in: .whitespaces)
                        : transcript

                    if !newPart.isEmpty {
                        self.latestTranscript = newPart
                        self.lastResultTime = Date()
                        self.isFinalTranscript = false
                        print("üîä [–ò–Ω—Ç–µ—Ä–≤—å—é–µ—Ä] –ù–æ–≤–∞—è —á–∞—Å—Ç—å: \(newPart)")
                        print("üîä [–ò–Ω—Ç–µ—Ä–≤—å—é–µ—Ä] –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: \(transcript)")
                    }
                }

                // Apple –æ—Ç–º–µ—á–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if result.isFinal && !self.isFinalTranscript {
                    self.isFinalTranscript = true
                    print("‚úÖ [–ò–Ω—Ç–µ—Ä–≤—å—é–µ—Ä] –§—Ä–∞–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (SFSpeech)")
                }
            }

            if let error = error {
                // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º "No speech detected" - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∫–æ–≥–¥–∞ –º–æ–ª—á–∞—Ç
                let errorMessage = error.localizedDescription
                if !errorMessage.contains("No speech detected") {
                    print("‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∞—É–¥–∏–æ: \(errorMessage)")
                }
            }
        }
    }

    func getLatestTranscript() -> (text: String, fullText: String, isFinal: Bool) {
        return (latestTranscript, fullAccumulatedText, isFinalTranscript)
    }

    func clearTranscript() {
        latestTranscript = ""
        fullAccumulatedText = ""
        isFinalTranscript = false
        lastResultTime = nil

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –º–µ—Ç–æ–¥ restartRecognition –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        restartRecognition()

        print("üßπ –°–∏—Å—Ç–µ–º–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –æ—á–∏—â–µ–Ω")
    }
}

// MARK: - SCStreamDelegate

extension SystemAudioCaptureService: SCStreamDelegate {
    func stream(_ stream: SCStream, didStopWithError error: Error) {
        print("‚ùå Stream –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Å –æ—à–∏–±–∫–æ–π: \(error)")
    }
}

// MARK: - SCStreamOutput

extension SystemAudioCaptureService: SCStreamOutput {
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
        // –¢–û–õ–¨–ö–û –∞—É–¥–∏–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é
        guard type == .audio else {
            return
        }

        // –ü–µ—Ä–µ–¥–∞—Ç—å –∞—É–¥–∏–æ –±—É—Ñ–µ—Ä –≤ Speech Recognition
        recognitionRequest?.appendAudioSampleBuffer(sampleBuffer)
    }
}
